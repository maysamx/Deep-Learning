{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m\n\u001b[0;32m     32\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Read the CSV file\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCleaned_Data joan output 1gb.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Find the maximum index value from both columns\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#max_word_index = max(data['Text_Encoded_Word'].apply(max).max(), data['Text_Encoded_Character'].apply(max).max())\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#print(max_word_index)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "##testing Attention(Layer)...bidirectional_lstm_gru_model\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set the seed ffor random number generation\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###balanced input 50%-50% and improved model ++++\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('Cleaned_Data joan output 1gb.csv')\n",
    "# Find the maximum index value from both columns\n",
    "#max_word_index = max(data['Text_Encoded_Word'].apply(max).max(), data['Text_Encoded_Character'].apply(max).max())\n",
    "#print(max_word_index)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('Cleaned_Data joan output 1gb.csv')\n",
    "\n",
    "# Split the dataset into scam and non-scam dataframes\n",
    "scam_data = data[data['Scam'] == 1]\n",
    "non_scam_data = data[data['Scam'] == 0]\n",
    "\n",
    "# Find the minimum number of samples between scam and non-scam dataframes\n",
    "min_samples = min(len(scam_data), len(non_scam_data))\n",
    "\n",
    "# Calculate the number of samples for each class to make 10% of the main dataset\n",
    "sample_size = int(1 * len(data) / 2)\n",
    "\n",
    "# Cap the sample size to the minimum number of samples if it exceeds the available samples in either class\n",
    "sample_size = min(sample_size, min_samples)\n",
    "\n",
    "# Randomly sample the specified number of rows from each dataframe\n",
    "sampled_scam_data = scam_data.sample(sample_size, random_state=42)\n",
    "sampled_non_scam_data = non_scam_data.sample(sample_size, random_state=42)\n",
    "\n",
    "# Concatenate the two sampled dataframes to create a balanced dataset\n",
    "balanced_data = pd.concat([sampled_scam_data, sampled_non_scam_data], axis=0)\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle the dataset\n",
    "\n",
    "# Print the number of samples taken from each class\n",
    "print(f\"Number of scam samples: {len(sampled_scam_data)}\")\n",
    "print(f\"Number of non-scam samples: {len(sampled_non_scam_data)}\")\n",
    "\n",
    "# Print the total number of samples in the balanced_data DataFrame\n",
    "print(f\"Total number of samples in the balanced dataset: {len(balanced_data)}\")\n",
    "\n",
    "\n",
    "# Split the balanced dataset into training and testing sets, maintaining the 50-50 balance\n",
    "train_data, test_data = train_test_split(balanced_data, test_size=0.2, random_state=42, stratify=balanced_data['Scam'])\n",
    "\n",
    "# Calculate the percentage of scam and non-scam samples in the train_data\n",
    "train_scam_count = train_data['Scam'].sum()\n",
    "train_total_count = len(train_data)\n",
    "train_scam_percentage = (train_scam_count / train_total_count) * 100\n",
    "train_non_scam_percentage = 100 - train_scam_percentage\n",
    "\n",
    "print(f\"Training data: Scam = {train_scam_percentage:.2f}%, Non-Scam = {train_non_scam_percentage:.2f}%\")\n",
    "\n",
    "# Calculate the percentage of scam and non-scam samples in the test_data\n",
    "test_scam_count = test_data['Scam'].sum()\n",
    "test_total_count = len(test_data)\n",
    "test_scam_percentage = (test_scam_count / test_total_count) * 100\n",
    "test_non_scam_percentage = 100 - test_scam_percentage\n",
    "\n",
    "print(f\"Testing data: Scam = {test_scam_percentage:.2f}%, Non-Scam = {test_non_scam_percentage:.2f}%\")\n",
    "\n",
    "# train_data, test_data are already defined when you split the balanced_data earlier\n",
    "\n",
    "# Prepare the data\n",
    "max_length = 100  # Set your desired max length for padding\n",
    "max_words = 1700000  # Set the maximum number of words in the vocabulary\n",
    "\n",
    "\n",
    "import ast\n",
    "\n",
    "train_data['Text_Encoded_Word'] = train_data['Text_Encoded_Word'].apply(ast.literal_eval)\n",
    "test_data['Text_Encoded_Word'] = test_data['Text_Encoded_Word'].apply(ast.literal_eval)\n",
    "\n",
    "train_data['Text_Encoded_Character'] = train_data['Text_Encoded_Character'].apply(ast.literal_eval)\n",
    "test_data['Text_Encoded_Character'] = test_data['Text_Encoded_Character'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "# Process 'Text_Encoded_Word' column,, 'Text_Encoded_Word' column, which contains tokenized text at the word level\n",
    "X_train_word = pad_sequences(train_data['Text_Encoded_Word'].tolist(), maxlen=max_length)\n",
    "X_test_word = pad_sequences(test_data['Text_Encoded_Word'].tolist(), maxlen=max_length)\n",
    "\n",
    "# Process 'Text_Encoded_Character' column,,contains tokenized text at the character level,\n",
    "X_train_char = pad_sequences(train_data['Text_Encoded_Character'].tolist(), maxlen=max_length)\n",
    "X_test_char = pad_sequences(test_data['Text_Encoded_Character'].tolist(), maxlen=max_length)\n",
    "\n",
    "# Process 'Scam' column\n",
    "y_train = train_data['Scam'].values\n",
    "y_test = test_data['Scam'].values\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, BatchNormalization, GlobalMaxPooling1D, Dense, Activation, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.layers import SpatialDropout1D, MaxPooling1D\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Activation, Dropout\n",
    "\n",
    "from keras.layers import LSTM, GRU\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D, Bidirectional, LSTM, GRU, Dense\n",
    "\n",
    "\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"attention_weight\",\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"attention_bias\",\n",
    "                                 shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
    "        at = K.softmax(et)\n",
    "        at = K.expand_dims(at, axis=-1)\n",
    "        output = x * at\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(Attention, self).get_config()\n",
    "\n",
    "\n",
    "\n",
    "def bidirectional_lstm_gru_model(embedding_matrix=None):\n",
    "    model = Sequential()\n",
    "\n",
    "    if embedding_matrix is None:\n",
    "        model.add(Embedding(max_words, 200, input_length=max_length))\n",
    "    else:\n",
    "        model.add(Embedding(max_words, 200, input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "    # Add bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "\n",
    "    # Add bidirectional GRU layer\n",
    "    model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
    "\n",
    "    # Add attention layer\n",
    "    model.add(Attention())\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = bidirectional_lstm_gru_model()\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "#history = model.fit(input_data[model_type]['input'], y_train, validation_data=(input_data[model_type]['input'], y_test), epochs=input_data[model_type]['epochs'], batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define a list containing dictionaries for word-level and character-level input data\n",
    "input_data = [\n",
    "    {\n",
    "        \"name\": \"Text_Encoded_Word\",\n",
    "        \"X_train\": X_train_word,\n",
    "        \"X_test\": X_test_word,\n",
    "        \"epochs\": 3  # Set the number of epochs for Text_Encoded_Word\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Text_Encoded_Character\",\n",
    "        \"X_train\": X_train_char,\n",
    "        \"X_test\": X_test_char,\n",
    "        \"epochs\": 10  # Set the number of epochs for Text_Encoded_Character\n",
    "    }\n",
    "]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for data in input_data:\n",
    "    print(f\"Training model on {data['name']} column\")\n",
    "    # Assign X_train and X_test from the current input_data dictionary\n",
    "    X_train = data[\"X_train\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = bidirectional_lstm_gru_model()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with the corresponding number of epochs\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=data[\"epochs\"], batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(data['X_test'], y_test)\n",
    "    print(f\"Test accuracy on {data['name']} column: {accuracy}\")\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    y_pred = np.round(model.predict(data['X_test']))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # Print the confusion matrix along with an explanation\n",
    "    print(f\"Confusion matrix on {data['name']} column:\\n {cm}\\n\")\n",
    "    print(\"Confusion Matrix Explanation:\")\n",
    "    print(\"True Negative (TN):\", cm[0][0], \"Non-scam tweets correctly classified as non-scam\")\n",
    "    print(\"False Positive (FP):\", cm[0][1], \"Non-scam tweets incorrectly classified as scam\")\n",
    "    print(\"False Negative (FN):\", cm[1][0], \"Scam tweets incorrectly classified as non-scam\")\n",
    "    print(\"True Positive (TP):\", cm[1][1], \"Scam tweets correctly classified as scam\\n\")\n",
    "\n",
    "    # Create a single figure with two subplots: one for accuracy and one for loss\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Plot training & validation accuracy values on the first subplot\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_title(f'Model accuracy for {data[\"name\"]} column')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values on the second subplot\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_title(f'Model loss for {data[\"name\"]} column')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(['Train', 'Test'], loc='upper left')\n",
    "    \n",
    "    # Show the figure\n",
    "    plt.show()\n",
    "\n",
    "    # Normalize accuracy and loss values\n",
    "    train_acc_norm = [acc / max(history.history['accuracy']) for acc in history.history['accuracy']]\n",
    "    test_acc_norm = [acc / max(history.history['val_accuracy']) for acc in history.history['val_accuracy']]\n",
    "    train_loss_norm = [loss / max(history.history['loss']) for loss in history.history['loss']]\n",
    "    test_loss_norm = [loss / max(history.history['val_loss']) for loss in history.history['val_loss']]\n",
    "\n",
    "    # Create a single figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot training & validation accuracy values on the same plot\n",
    "    ax.plot(train_acc_norm, 'b', label='Training accuracy')\n",
    "    ax.plot(test_acc_norm, 'b--', label='Testing accuracy')\n",
    "\n",
    "    # Plot training & validation loss values on the same plot\n",
    "    ax.plot(train_loss_norm, 'r', label='Training loss')\n",
    "    ax.plot(test_loss_norm, 'r--', label='Testing loss')\n",
    "\n",
    "    # Set title, labels, and legend\n",
    "    ax.set_title(f'Model accuracy and loss for {data[\"name\"]} column')\n",
    "    ax.set_ylabel('Normalized Accuracy / Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend()\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
